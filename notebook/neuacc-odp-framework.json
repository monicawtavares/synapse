{
	"name": "neuacc-odp-framework",
	"properties": {
		"folder": {
			"name": "DataPlatform/logging"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "8b942724-8965-4ca5-82c9-601fdcb3bbb7"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import *\n",
					"from delta.tables import *\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql.functions import expr\n",
					"import numpy as np\n",
					"import datetime\n",
					"import pytz\n",
					"import concurrent.futures\n",
					"import networkx as nx"
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class metadataReader:\n",
					"    \"\"\"\n",
					"    Class for reading metadata information from a schema and performing various queries related to table definitions,\n",
					"    connection definitions, and ingestion processes for lakehouse layers.\n",
					"    \"\"\"\n",
					"\n",
					"    def __init__(self, metadata_schema):\n",
					"        \"\"\"\n",
					"        Initialize the metadataReader with the provided schema.\n",
					"\n",
					"        Args:\n",
					"            metadata_schema (str): The schema to be used for querying metadata tables.\n",
					"        \"\"\"\n",
					"        self.schema = metadata_schema\n",
					"\n",
					"    def table_def(self, table_category, table_name, layer):\n",
					"        \"\"\"\n",
					"        Retrieve the table definition for a specific table in a given category and lakehouse layer.\n",
					"\n",
					"        Args:\n",
					"            table_category (str): The category of the target table.\n",
					"            table_name (str): The name of the target table.\n",
					"            layer (str): The lakehouse layer (e.g., silver, gold) where the table resides.\n",
					"\n",
					"        Returns:\n",
					"            dict: A dictionary containing column names and their corresponding values from the metadata table.\n",
					"        \"\"\"\n",
					"        table_def_dict = dict()\n",
					"        metadata_df = spark.sql(f\"\"\"Select * from {self.schema}.metadata_table_definition where lower(Lakehouse_Layer) = '{layer}' and lower(Target_Table_Category) = '{table_category}' \n",
					"                                and lower(Target_Table_Name) = '{table_name}'\"\"\")\n",
					"        for i in metadata_df.columns:\n",
					"            table_def_dict[f'{i}'] = metadata_df.select(f'{i}').first()[0]\n",
					"        return table_def_dict\n",
					"\n",
					"    def connection_def(self, connection_id):\n",
					"        \"\"\"\n",
					"        Retrieve the connection definition based on the connection ID.\n",
					"\n",
					"        Args:\n",
					"            connection_id (str): The ID of the connection.\n",
					"\n",
					"        Returns:\n",
					"            dict: A dictionary containing connection metadata based on the provided connection ID.\n",
					"        \"\"\"\n",
					"        connection_def_dict = dict()\n",
					"        metadata_df = spark.sql(f\"\"\"Select * from {self.schema}.metadata_connection_definition where Connection_ID = '{connection_id}'\"\"\")\n",
					"        for i in metadata_df.columns:\n",
					"            connection_def_dict[f'{i}'] = metadata_df.select(f'{i}').first()[0]\n",
					"        return connection_def_dict\n",
					"    \n",
					"    def db_ingest(self, table_name,table_category, target_catalog):\n",
					"        \"\"\"\n",
					"        Retrieve database ingestion details for a specific table in a given category and catalog.\n",
					"\n",
					"        Args:\n",
					"            table_name (str): The name of the source table.\n",
					"            table_category (str): The category of the table.\n",
					"            target_catalog (str): The target catalog for the table.\n",
					"\n",
					"        Returns:\n",
					"            dict: A dictionary containing database ingestion metadata.\n",
					"        \"\"\"\n",
					"        db_ingest_dict = dict()\n",
					"        metadata_df = spark.sql(f\"\"\"Select * from {self.schema}.metadata_database_ingestion where lower(Target_Table_Category) = '{table_category}' and \n",
					"                                lower(Source_Table_Name) = '{table_name}' \n",
					"                                and lower(Target_Catalog) = '{target_catalog}' and lower(Ingest_Flag) = 'y'  \"\"\")\n",
					"        for i in metadata_df.columns:\n",
					"            db_ingest_dict[f'{i}'] = metadata_df.select(f'{i}').first()[0]\n",
					"        return db_ingest_dict\n",
					"\n",
					"    def file_ingest(self, table_name, table_category, target_catalog):\n",
					"        \"\"\"\n",
					"        Retrieve file ingestion details for a specific table in a given category and catalog.\n",
					"\n",
					"        Args:\n",
					"            table_name (str): The name of the source table.\n",
					"            table_category (str): The category of the table.\n",
					"            target_catalog (str): The target catalog for the table.\n",
					"\n",
					"        Returns:\n",
					"            dict: A dictionary containing file ingestion metadata.\n",
					"        \"\"\"\n",
					"        file_ingest_dict = dict()\n",
					"        metadata_df = spark.sql(f\"\"\"Select * from {self.schema}.metadata_file_ingestion where lower(Target_Table_Category) = '{table_category}' \n",
					"                                and lower(Source_Table_Name) = '{table_name}' and lower(Target_Catalog) = '{target_catalog}' and lower(Ingest_Flag) = 'y' \"\"\")\n",
					"        \n",
					"        for i in metadata_df.columns:\n",
					"            file_ingest_dict[f'{i}'] = metadata_df.select(f'{i}').first()[0]\n",
					"        return file_ingest_dict\n",
					"\n",
					"    def get_lakehouse(self, table_name, table_category, silver_lakehouse, gold_lakehouse):\n",
					"        \"\"\"\n",
					"        Retrieve the lakehouse layer (silver or gold) for a given table in a specific category.\n",
					"\n",
					"        Args:\n",
					"            table_name (str): The name of the target table.\n",
					"            table_category (str): The category of the target table.\n",
					"            silver_lakehouse (str): The name of the silver lakehouse.\n",
					"            gold_lakehouse (str): The name of the gold lakehouse.\n",
					"\n",
					"        Returns:\n",
					"            str: The name of the final lakehouse (either silver or gold).\n",
					"        \"\"\"\n",
					"        lakehouse_layer = spark.sql(f\"\"\"Select Lakehouse_Layer from {self.schema}.metadata_table_definition where lower(Target_Table_Category) = '{table_category}'\n",
					"                                    and lower(Target_Table_Name) = '{table_name}'\"\"\").first()[0]\n",
					"        if lakehouse_layer.lower() == 'gold':\n",
					"            final_layer = gold_lakehouse\n",
					"        elif lakehouse_layer.lower() == 'silver':\n",
					"            final_layer = silver_lakehouse\n",
					"        return final_layer\n",
					"\n",
					"    def get_bronze_tables(self, source_type, table_category, target_catalog):\n",
					"        \"\"\"\n",
					"        Retrieve the list of bronze tables based on the source type and catalog.\n",
					"\n",
					"        Args:\n",
					"            source_type (str): The source type, either 'database' or 'file'.\n",
					"            table_category (str): The category of the target table.\n",
					"            target_catalog (str): The target catalog.\n",
					"\n",
					"        Returns:\n",
					"            list: A list of bronze tables.\n",
					"        \"\"\"\n",
					"        if source_type.lower() == 'database':\n",
					"            metadata_df = spark.sql(f\"\"\"SELECT Source_Table_Name from {self.schema}.metadata_database_ingestion where lower(Target_Table_Category) = '{table_category}' and\n",
					"                                    lower(Target_Catalog) = '{target_catalog}' and lower(Ingest_Flag) = 'y' \"\"\")\n",
					"        else:\n",
					"            metadata_df = spark.sql(f\"\"\"SELECT Source_Table_Name from {self.schema}.metadata_file_ingestion where lower(Target_Table_Category) = '{table_category}' and\n",
					"                        lower(Target_Catalog) = '{target_catalog}' and lower(Ingest_Flag) = 'y' \"\"\")\n",
					"            \n",
					"        bronze_table_list = metadata_df.select('Source_Table_Name').toPandas()['Source_Table_Name'].tolist()\n",
					"        return bronze_table_list\n",
					"\n",
					"    def get_silver_tables(self,silver_category):\n",
					"        \"\"\"\n",
					"        Retrieve the list of silver tables for a given category.\n",
					"\n",
					"        Args:\n",
					"            silver_category (str): The category of the silver tables.\n",
					"\n",
					"        Returns:\n",
					"            list: A list of silver tables with their full names.\n",
					"        \"\"\"\n",
					"        metadata_df = spark.read.format('delta').table(f'{self.schema}.metadata_table_definition')\n",
					"        silver_tables = metadata_df.where(f\"\"\"lower(Target_Table_Category) = '{silver_category}' and lower(Lakehouse_Layer) = 'silver'\"\"\").select(\"Target_Table_Name\").toPandas()['Target_Table_Name'].tolist()\n",
					"        silver_table_list = [f\"odpSilver.{silver_category}_{table}\" for table in silver_tables]\n",
					"        return silver_table_list\n",
					"\n",
					"    def get_gold_tables(self,gold_category,dim_type):\n",
					"        \"\"\"\n",
					"        Retrieve the list of gold tables for a given category and dimension type.\n",
					"\n",
					"        Args:\n",
					"            gold_category (str): The category of the gold tables.\n",
					"            dim_type (str): The dimension type of the gold tables.\n",
					"\n",
					"        Returns:\n",
					"            list: A list of gold tables with their full names.\n",
					"        \"\"\"\n",
					"        metadata_df = spark.read.format('delta').table(f'{self.schema}.metadata_table_definition')\n",
					"        gold_tables = metadata_df.where(f\"\"\"lower(Target_Table_Category) = '{gold_category}' and lower(Dimension_Type) = '{dim_type}' \n",
					"                                            and lower(Lakehouse_Layer) = 'gold'\"\"\").select(\"Target_Table_Name\").toPandas()['Target_Table_Name'].tolist()\n",
					"        gold_table_list = [f\"odpGold.{gold_category}_{table}\" for table in gold_tables]\n",
					"        return gold_table_list\n",
					"\n",
					"    def get_category_order(self,target_table_category,bronzeLakehouse,silverLakehouse,goldLakehouse):\n",
					"        \"\"\"\n",
					"        Retrieve the category order based on the table category and lakehouses.\n",
					"\n",
					"        Args:\n",
					"            target_table_category (str): The category of the target table.\n",
					"            bronzeLakehouse (str): The bronze lakehouse name.\n",
					"            silverLakehouse (str): The silver lakehouse name.\n",
					"            goldLakehouse (str): The gold lakehouse name.\n",
					"\n",
					"        Returns:\n",
					"            list: A nested list containing category orders for bronze, silver, and gold lakehouses.\n",
					"        \"\"\"\n",
					"        df = spark.read.format('delta').table(f'{self.schema}.metadata_table_dependency')\n",
					"        order = df.where(f\"\"\"Table_Category = '{target_table_category}'\"\"\").select(collect_set(\"Category_Order\")).collect()[0][0]\n",
					"        if len(order) ==1:\n",
					"            category_list = order[0]\n",
					"            category_order = [[category] for category in category_list]\n",
					"        else:\n",
					"            bronze_list = []\n",
					"            silver_list = []\n",
					"            gold_list = []\n",
					"            for sublist in order:\n",
					"                for category in sublist:\n",
					"                    if bronzeLakehouse.lower() in category.lower():\n",
					"                        bronze_list.append(category)\n",
					"                    elif silverLakehouse.lower() in category.lower():\n",
					"                        silver_list.append(category)\n",
					"                    elif goldLakehouse.lower() in category.lower():\n",
					"                        gold_list.append(category)\n",
					"                category_order = [list(set(bronze_list)),list(set(silver_list)),list(set(gold_list))]\n",
					"        return category_order\n",
					"\n",
					"    def get_profisee_tables(self,table_category):\n",
					"        \"\"\"\n",
					"        Retrieve the list of Profisee tables for a given category.\n",
					"\n",
					"        Args:\n",
					"            table_category (str): The category of the Profisee tables.\n",
					"\n",
					"        Returns:\n",
					"            list: A list of Profisee tables and their corresponding entities.\n",
					"        \"\"\"\n",
					"        metadata_df = spark.sql(f\"\"\"Select Target_Table_Name, Profisee_Entity from {self.schema} where lower(Lakehouse_Layer) = 'silver' \n",
					"                                    and lower(Target_Table_Category) = '{table_category}' and lower(Profisee_Flag) in ('yes','y')\"\"\")\n",
					"        profisee_tables = metadata_df.toPandas().values.tolist()\n",
					"        return profisee_tables\n",
					"    \n",
					"    def get_profisee_info(self, table_category, table_name):\n",
					"        metadata_df = spark.sql(f\"\"\"Select * from {self.schema}.metadata_table_definition where lower(Lakehouse_Layer) = 'silver' and lower(Target_Table_Category) = '{table_category}'\n",
					"                                    and lower(Target_Table_Name) = '{table_name}' and lower(Profisee_Flag) in ('yes','y')\"\"\")\n",
					"        profisee_dict = dict()\n",
					"        for i in metadata_df.columns:\n",
					"            profisee_dict[f'{i}'] = metadata_df.select(f'{i}').first()[0]\n",
					"        return profisee_dict"
				],
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Build_JDBC_Connection:\n",
					"    def __init__(self,connection_system_type,connection_server_name,port_number,username,password,db_database_name):\n",
					"        self.connection_system_type = connection_system_type\n",
					"        self.connection_server_name = connection_server_name\n",
					"        self.port_number = port_number\n",
					"        self.username = username\n",
					"        self.password = password\n",
					"        self.db_database_name = db_database_name\n",
					"\n",
					"    def build_url(self):\n",
					"        if self.connection_system_type.lower() == \"sqlserver\":\n",
					"            #return \"jdbc:sqlserver://\" + self.connection_server_name + f\":{self.port_number};database=\" + self.db_database_name + \";user=\" + self.username+\";password=\" + self.password\n",
					"            return f\"jdbc:sqlserver://{self.connection_server_name}:{self.port_number};database={self.db_database_name};user={self.username};password={self.password}\"\n",
					"        elif self.connection_system_type.lower() == \"postgresql\":\n",
					"            #return \"jdbc:postgresql://\" + self.connection_server_name + f\":{self.port_number}/\" +  self.db_database_name + \"?user=\" + self.username + \"&password=\" + self.password\n",
					"            return f\"jdbc:postgresql://{self.connection_server_name}:{self.port_number}/{self.db_database_name}?user={self.username}&password={self.password}\"\n",
					"        elif self.connection_system_type.lower() == \"db2\":\n",
					"            return f\"jdbc:db2://{self.connection_server_name}:{self.port}/{self.db_database_name}:user={self.username};password={self.password}\"\n",
					"        elif self.connection_system_type.lower() == \"mariadb\":\n",
					"            return f\"jdbc:mariadb://{self.connection_server_name}:{self.port}/{self.db_database_name}?user={self.username}&password={self.password}\"\n",
					"        elif self.connection_system_type.lower() == \"oracle\":\n",
					"            #return f\"jdbc:oracle:thin:@{self.connection_server_name}:{self.port}:{self.db_database_name}\"\n",
					"            return f\"jdbc:oracle:thin@{self.connection_server_name}:{self.port_number}/{self.db_database_name};user={self.username};password={self.password}\"\n",
					"        elif self.connection_system_type.lower() == \"mysql\":\n",
					"            return f\"jdbc:mysql://{self.username}:{self.password}@{self.connection_server_name}:{self.port_number}\"\n",
					"        else:\n",
					"            print(\"Unsupported database type\")\n",
					"\n",
					"    def build_driver(self):\n",
					"        if self.connection_system_type.lower() == \"sqlserver\":\n",
					"            return 'com.microsoft.sqlserver.jdbc.SQLServerDriver'\n",
					"        elif self.connection_system_type.lower() == \"postgresql\":\n",
					"            return \"org.postgresql.Driver\"\n",
					"        elif self.connection_system_type.lower() == \"db2\":\n",
					"            return \"com.ibm.db2.jcc.DB2Driver\"\n",
					"        elif self.connection_system_type.lower() == \"mariadb\":\n",
					"            return \"org.mariadb.jdbc.Driver\"\n",
					"        elif self.connection_system_type.lower() == \"oracle\":\n",
					"            return \"oracle.jdbc.OracleDriver\"\n",
					"        elif self.connection_system_type.lower() == \"mysql\":\n",
					"            return \"com.mysql.cj.jdbc.Driver\"\n",
					"        else:\n",
					"            print(\"Unsupported database type\")"
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_layer(lakehouse_name):\n",
					"    \"\"\"\n",
					"    Determine the layer (silver, gold, or bronze) based on the provided lakehouse name.\n",
					"\n",
					"    Args:\n",
					"        lakehouse_name (str): The name of the lakehouse.\n",
					"\n",
					"    Returns:\n",
					"        str: The corresponding layer of the lakehouse ('silver', 'gold', or 'bronze').\n",
					"    \n",
					"    Notes:\n",
					"        - If the lakehouse name matches the silver lakehouse, the layer is 'silver'.\n",
					"        - If the lakehouse name matches the gold lakehouse, the layer is 'gold'.\n",
					"        - If neither, the layer defaults to 'bronze'.\n",
					"    \"\"\"\n",
					"    if lakehouse_name.lower() == silver_lakehouse.lower():\n",
					"        layer = 'silver'\n",
					"    elif lakehouse_name.lower() == gold_lakehouse.lower():\n",
					"        layer = 'gold'\n",
					"    else:\n",
					"        layer = 'bronze'\n",
					"    return layer"
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Function used to determine whether a given source table is file based or comes from a database. Function is called from the master notebook to determine which source notebook, file-ingestion or database-ingestion, needs to be called.\n",
					"\n",
					"def file_or_db(metadata_schema, target_table_category, current_catalog, target_table_name=None):\n",
					"    \"\"\"\n",
					"    Determines whether the ingestion for a target table category in a given catalog is from a file, a database, or both.\n",
					"\n",
					"    Args:\n",
					"        metadata_schema (str): The schema that contains metadata tables for file and database ingestion.\n",
					"        target_table_category (str): The category of the target table.\n",
					"        current_catalog (str): The catalog being queried.\n",
					"        target_table_name (str, optional): The name of the target table (optional). If provided, the function checks the \n",
					"                                           ingestion method for this specific table. If not provided, it checks ingestion for the whole category.\n",
					"\n",
					"    Returns:\n",
					"        str: A string indicating the ingestion method:\n",
					"             - 'file' if the data is ingested from files.\n",
					"             - 'database' if the data is ingested from a database.\n",
					"             - 'both' if data is ingested from both files and databases.\n",
					"    \"\"\"\n",
					"    if target_table_name is None:\n",
					"        file_df = spark.sql(f\"\"\"Select * from {metadata_schema}.metadata_file_ingestion where lower(Target_Table_Category) = '{target_table_category.lower()}'\n",
					"                            and lower(Target_Catalog) = '{current_catalog}'\"\"\")\n",
					"        database_df = spark.sql(f\"\"\"Select * from {metadata_schema}.metadata_database_ingestion where lower(Target_Table_Category) = '{target_table_category.lower()}'\n",
					"                                and lower(Target_Catalog) = '{current_catalog}'\"\"\")\n",
					"        \n",
					"        if not file_df.isEmpty() and not database_df.isEmpty():\n",
					"            table = 'both'\n",
					"        elif file_df.isEmpty():\n",
					"            table = 'database'\n",
					"        else:\n",
					"            table = 'file'\n",
					"    else:\n",
					"        file_df = spark.sql(f\"\"\"Select * from {metadata_schema}.metadata_file_ingestion where lower(Target_Table_Category) = '{target_table_category.lower()}' \n",
					"                        and lower(Source_Table_Name) = '{target_table_name.lower()}' and lower(Target_Catalog) = '{current_catalog}' and lower(Ingest_Flag) = 'y' \"\"\")\n",
					"        if file_df.isEmpty():\n",
					"            table = 'database'\n",
					"        else:\n",
					"            table = 'file'\n",
					"    return table"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def create_table_dependency(metadata_schema):\n",
					"    \"\"\"\n",
					"    Creates the `metadata_table_dependency` table in the specified schema if it does not already exist.\n",
					"\n",
					"    The table contains columns for table category, table name, the order in which tables should be processed,\n",
					"    and the order of categories. The table is created using Delta Lake.\n",
					"\n",
					"    Args:\n",
					"        metadata_schema (str): The schema where the `metadata_table_dependency` table will be created.\n",
					"    \n",
					"    Returns:\n",
					"        None: The function prints a message once the table is created.\n",
					"    \"\"\"\n",
					"    query = f\"\"\"CREATE TABLE IF NOT EXISTS {metadata_schema}.metadata_table_dependency\n",
					"                (\n",
					"                    Table_Category String,\n",
					"                    Table_Name String,\n",
					"                    Table_Order Array<Array<String>>,\n",
					"                    Category_Order Array<String>\n",
					"                ) USING DELTA \"\"\"\n",
					"    spark.sql(query)\n",
					"    print('created table_dependency metadata table')"
				],
				"execution_count": 36
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_fabric_workspace():\n",
					"    \"\"\"\n",
					"    Retrieve the name of the corresponding Fabric workspace based on the current Synapse workspace.\n",
					"\n",
					"    The function uses the current Synapse workspace name and maps it to a predefined Fabric workspace name.\n",
					"\n",
					"    Returns:\n",
					"        str: The Fabric workspace name corresponding to the current Synapse workspace. \n",
					"             Returns None if the workspace name is not found in the mapping.\n",
					"    \"\"\"\n",
					"    synapse_ws = mssparkutils.env.getWorkspaceName()\n",
					"    fabric_synapse_map = {\n",
					"        \"chemdeveussyn\": \"CDP_Framework_Dev\",\n",
					"        \"chemtesteussyn\": \"CDP_Framework_Test\",\n",
					"        \"chemprodeussyn\": \"CDP_Framework_Prod\"\n",
					"    }\n",
					"    return fabric_synapse_map.get(synapse_ws)\n",
					"\n",
					"def get_current_env():\n",
					"    \"\"\"\n",
					"    Retrieve the current environment (dev, qa, or prod) based on the current Synapse workspace.\n",
					"\n",
					"    The function maps the current Synapse workspace to its corresponding environment name.\n",
					"\n",
					"    Returns:\n",
					"        str: The environment name corresponding to the current Synapse workspace (dev, qa, prod).\n",
					"             Returns None if the workspace name is not found in the mapping.\n",
					"    \"\"\"\n",
					"    synapse_ws = mssparkutils.env.getWorkspaceName()\n",
					"    env_synapse_map = {\n",
					"        \"chemdeveussyn\": \"dev\",\n",
					"        \"chemtesteussyn\": \"test\",\n",
					"        \"chemprodeussyn\": \"prod\",\n",
					"\n",
					"    }\n",
					"    return env_synapse_map.get(synapse_ws)"
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def table_def(table_category, table_name):\n",
					"        \"\"\"\n",
					"        Retrieves table definition details from the metadata for a specified table category and table name.\n",
					"\n",
					"        Parameters:\n",
					"        table_category (str): The category of the table (e.g., staging, operational).\n",
					"        table_name (str): The specific name of the target table.\n",
					"\n",
					"        Returns:\n",
					"        dict: A dictionary containing table metadata, with column names as keys and column values as values.\n",
					"        \"\"\"\n",
					"        table_def_dict = dict()\n",
					"        metadata_df = spark.sql(f\"\"\"Select * from {metadata_schema}.metadata_table_definition where lower(Target_Table_Name) = '{table_name}' and lower(Target_Table_Category) like '%{target_table_category.lower()}%'\"\"\")\n",
					"        for i in metadata_df.columns:\n",
					"            table_def_dict[f'{i}'] = metadata_df.select(f'{i}').first()[0]\n",
					"        return table_def_dict\n",
					"\n",
					"def add_table_dependency(metadata_schema, target_table_category, target_table_name):\n",
					"    \"\"\"\n",
					"    Adds a table dependency entry for the specified table into the metadata dependency table.\n",
					"    Builds the dependency graph and determines the order of table processing.\n",
					"\n",
					"    Parameters:\n",
					"    metadata_schema (str): The schema where the metadata tables are stored.\n",
					"    target_table_category (str): The category of the target table (e.g., sanctioned, staging).\n",
					"    target_table_name (str): The specific name of the target table.\n",
					"\n",
					"    Returns:\n",
					"    list: A list representing the order of table processing based on dependencies.\n",
					"    \"\"\"\n",
					"    table_name = f'{target_table_category}_sanctioneddb.{target_table_name}'\n",
					"    print(f'Adding {table_name} to metadata_table_dependency')\n",
					"    tableGraph = nx.DiGraph()\n",
					"    \n",
					"    add_dependencies(tableGraph, target_table_category, target_table_name, metadata_schema)\n",
					"\n",
					"    tableOrder = [sorted(tables) for tables in nx.topological_generations(tableGraph)]\n",
					"    arr=spark.sql(f\"select concat(target_table_category,'.',target_table_name) as name from {metadata_schema}.metadata_table_definition where lower(Target_Table_Category) like '%{target_table_category.lower()}%' AND DDL like '%Hydration_Script%'\").collect()\n",
					"    if len(arr)>1 and target_table_name.lower()==(target_name_list[-1]).lower():\n",
					"        tableOrder= tableOrder+[[x.name.lower() for x in arr]]\n",
					"    categoryOrder = [target_table_category]\n",
					"\n",
					"    data = [(f\"{target_table_category}\", f\"{target_table_name}\", tableOrder, categoryOrder)]\n",
					"    columns = [\"Table_Category\", \"Table_Name\", \"Table_Order\", \"Category_Order\"]\n",
					"    df = spark.createDataFrame(data, columns)\n",
					"    df.write.format('delta').mode('append').saveAsTable(f'{metadata_schema}.metadata_table_dependency')\n",
					"\n",
					"    return tableOrder\n",
					"\n",
					"def add_dependencies(graph, table_category, table_name, metadata_schema):\n",
					"    \"\"\"\n",
					"    Recursively adds dependencies to a directed graph for a given table, allowing for table order determination.\n",
					"\n",
					"    Parameters:\n",
					"    graph (nx.DiGraph): The directed graph to which dependencies will be added.\n",
					"    table_category (str): The category of the table (e.g., sanctioned, staging).\n",
					"    table_name (str): The name of the table for which dependencies are identified.\n",
					"    metadata_schema (str): The schema where the metadata tables are stored.\n",
					"\n",
					"    Returns:\n",
					"    None\n",
					"    \"\"\"\n",
					"    processed_tables = set()\n",
					"    tables_to_process = [(f\"{table_category}_sanctioneddb\", table_name)]\n",
					"\n",
					"    while tables_to_process:\n",
					"        current_category, current_table = tables_to_process.pop()\n",
					"        table_full_name = f\"{current_category}.{current_table}\"\n",
					"        if table_full_name.lower() in processed_tables:\n",
					"            continue\n",
					"\n",
					"        processed_tables.add(table_full_name.lower())\n",
					"        graph.add_node(table_full_name)\n",
					"\n",
					"        dependencies_df = spark.sql(f\"\"\"\n",
					"            SELECT Depends_On\n",
					"            FROM {metadata_schema}.metadata_table_definition\n",
					"            WHERE lower(Target_Table_Category) = '{current_category.lower()}'\n",
					"            AND lower(Target_Table_Name) = '{current_table.lower()}'\n",
					"            AND DDL not like '%Hydration_Script%' \n",
					"        \"\"\")\n",
					"\n",
					"        if dependencies_df.first() is not None and dependencies_df.first()[0]:\n",
					"            dependencies = dependencies_df.first()[0].split(',')\n",
					"            print(f'Dependencies of {table_full_name}: {dependencies}')\n",
					"            for dependency in dependencies:\n",
					"                dependency = dependency.strip()\n",
					"                if not dependency:\n",
					"                    continue\n",
					"                graph.add_edge(dependency, table_full_name)\n",
					"                try:\n",
					"                    dep_layer_name, dep_table_name = dependency.split(\".\")\n",
					"                    dep_layer_name = dep_layer_name.strip()\n",
					"                    dep_table_name = dep_table_name.strip()\n",
					"                    full_dependency_name = f\"{dep_layer_name}.{dep_table_name}\"\n",
					"                    if full_dependency_name.lower() not in processed_tables:\n",
					"                        tables_to_process.append((dep_layer_name, dep_table_name))\n",
					"                except ValueError:\n",
					"                    print(f\"Invalid dependency format: {dependency}\")\n",
					"        else:\n",
					"            print(f\"No dependencies for {table_full_name}\")\n",
					"\n",
					"                \n",
					"def get_table_order(target_table_category,target_table_name,metadata_schema):\n",
					"        \"\"\"\n",
					"        Retrieves or calculates the processing order of tables based on dependencies for a specific category and table name.\n",
					"\n",
					"        Parameters:\n",
					"        target_table_category (str): The category of the target table.\n",
					"        target_table_name (str): The specific name of the target table.\n",
					"        metadata_schema (str): The schema where the metadata tables are stored.\n",
					"\n",
					"        Returns:\n",
					"        list: A list representing the order of tables to be processed.\n",
					"        \"\"\"\n",
					"        if target_table_name==\"\":\n",
					"            arr=spark.sql(f\"select concat(target_table_category,'.',target_table_name) as name from {metadata_schema}.metadata_table_definition where lower(Target_Table_Category) like '%{target_table_category.lower()}%'\").collect()\n",
					"            return [[x.name.lower() for x in arr]]\n",
					"\n",
					"\n",
					"        query =f\"\"\"Select Table_Order from {metadata_schema}.metadata_table_dependency where lower(Table_Category) like '%{target_table_category.lower()}%' and lower(Table_Name) = '{target_table_name.lower()}'\"\"\"\n",
					" \n",
					"        #f\"\"\"Select Table_Order from {metadata_schema}.metadata_table_dependency where lower(Table_Category) = '{target_table_category.lower()}_sanctioneddb' and lower(Table_Name) = '{target_table_name.lower()}'\"\"\"\n",
					"        process_order = spark.sql(query)\n",
					"\n",
					"        if not process_order.isEmpty():\n",
					"            table_order = process_order.first()[0]\n",
					"        else:\n",
					"            table_order = add_table_dependency(metadata_schema,target_table_category,target_table_name)\n",
					"            \n",
					"        return table_order"
				],
				"execution_count": 38
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def map_option_sets(source_df1, dictionary_df):\n",
					"    \"\"\"\n",
					"    Maps option set values in the `source_df1` DataFrame to their corresponding labels from `dictionary_df`.\n",
					" \n",
					"    This function takes an input DataFrame `source_df1`, identifies relevant option set columns, and replaces their\n",
					"    values with localized labels from `dictionary_df`. It handles unpivoting and re-pivoting of data to perform lookups.\n",
					" \n",
					"    Parameters:\n",
					"    ----------\n",
					"    source_df1 : pyspark.sql.DataFrame\n",
					"        DataFrame containing opportunity data, where columns represent option set values to be mapped.\n",
					" \n",
					"    dictionary_df : pyspark.sql.DataFrame\n",
					"        DataFrame with option set mappings, including columns 'OptionSetName', 'Option' (option set values),\n",
					"        and 'LocalizedLabel' (localized names for each option).\n",
					" \n",
					"    Returns:\n",
					"    -------\n",
					"    pyspark.sql.DataFrame\n",
					"        The modified `source_df1` DataFrame with additional columns that contain display names for option set values.\n",
					"        These columns are named in the format `{OptionSetName}_displayname`.\n",
					" \n",
					"    Notes:\n",
					"    ------\n",
					"    - If no option set columns are present in `source_df1`, the function returns the original `source_df1`.\n",
					"    - The process includes casting to string types, unpivoting `source_df1` for lookup, and re-pivoting to join back.\n",
					" \n",
					"    Example:\n",
					"    --------\n",
					"    Given `source_df1` with columns 'Id' and 'Status' (an option set value),\n",
					"    and `dictionary_df` with mappings for 'Status' values:\n",
					"   \n",
					"        source_df1:\n",
					"        +---+------+\n",
					"        | Id|Status|\n",
					"        +---+------+\n",
					"        |  1|   100|\n",
					"        |  2|   200|\n",
					"       \n",
					"        dictionary_df:\n",
					"        +-------------+--------+--------------+\n",
					"        |OptionSetName|Option  |LocalizedLabel|\n",
					"        +-------------+--------+--------------+\n",
					"        |      Status |    100 |    'Open'    |\n",
					"        |      Status |    200 |    'Closed'  |\n",
					" \n",
					"    The output will include a 'Status_displayname' column with mapped values.\n",
					" \n",
					"    \"\"\"\n",
					"    from pyspark.sql.functions import expr, col, first, lit, concat\n",
					"    from pyspark.sql import functions as F\n",
					"    # Get list of OptionSetNames that exist in both dictionary_df and source_df1 columns\n",
					"    option_set_names = [row['OptionSetName'] for row in dictionary_df.select('OptionSetName').distinct().collect()]\n",
					"    option_set_names = [name for name in option_set_names if name in source_df1.columns]\n",
					"    \n",
					"    if not option_set_names:\n",
					"        return source_df1\n",
					"\n",
					"    # Cast all relevant columns in source_df1 to string to ensure consistent data types\n",
					"    for col_name in option_set_names:\n",
					"        source_df1 = source_df1.withColumn(col_name, col(col_name).cast('string'))\n",
					"    \n",
					"    # Create the stack expression for unpivoting\n",
					"    expr_str = \", \".join([f\"'{col_name}', {col_name}\" for col_name in option_set_names])\n",
					"    \n",
					"    # Unpivot the source_df1\n",
					"    source_df2 = source_df1.select('Id', expr(f\"stack({len(option_set_names)}, {expr_str}) as (OptionSetName, OptionValue)\"))\n",
					"    \n",
					"    # Ensure OptionValue and Option are strings to prevent data type mismatches during join\n",
					"    source_df2 = source_df2.withColumn('OptionValue', col('OptionValue').cast('string'))\n",
					"    dictionary_df = dictionary_df.withColumn('Option', col('Option').cast('string'))\n",
					"    \n",
					"    # Use aliases to avoid ambiguity\n",
					"    opp_alias = source_df2.alias('opp')\n",
					"    dict_alias = dictionary_df.alias('dict')\n",
					"    \n",
					"    # Perform the join with qualified column names\n",
					"    joined_df = opp_alias.join(\n",
					"        dict_alias.select('OptionSetName', 'Option', 'LocalizedLabel'),\n",
					"        (col('opp.OptionSetName') == col('dict.OptionSetName')) &\n",
					"        (col('opp.OptionValue') == col('dict.Option')),\n",
					"        'left'\n",
					"    )\n",
					"    \n",
					"    # Create the display column name\n",
					"    joined_df = joined_df.withColumn('display_column_name', concat(col('opp.OptionSetName'), lit('_displayname')))\n",
					"    \n",
					"    # Pivot back to wide format\n",
					"    display_df = joined_df.groupBy('Id').pivot('display_column_name').agg(first('LocalizedLabel'))\n",
					"    \n",
					"    # Join the display_df back to the original source_df1\n",
					"    source_df1 = source_df1.join(display_df, on='Id', how='left')\n",
					"    \n",
					"    return source_df1\n",
					"\n",
					""
				],
				"execution_count": 39
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def process_bronze_tables(bronze_tables):\n",
					"    \"\"\"\n",
					"    Runs a notebook to process the given bronze table and returns the output log.\n",
					"\n",
					"    This function executes the ingestion notebook specified by `file_ingest_notebook_path` using the `mssparkutils.notebook.run` method. \n",
					"    It passes various parameters related to the table's metadata, ingestion pipeline, and overwrite behavior.\n",
					"\n",
					"    Parameters:\n",
					"    ----------\n",
					"    bronze_tables : str\n",
					"        The name of the bronze table to be processed.\n",
					"\n",
					"    Notebook Run Parameters:\n",
					"    ------------------------\n",
					"    - `useRootDefaultLakehouse`: Boolean flag indicating the use of the root default lakehouse.\n",
					"    - `metadata_schema`: The schema containing metadata information.\n",
					"    - `target_table_category`: The category of the target table, assumed to be defined elsewhere in the environment.\n",
					"    - `source_table_name`: The name of the source table, set to the value of `bronze_tables`.\n",
					"    - `pipeline_id`: The ID of the pipeline run, assumed to be defined elsewhere in the environment.\n",
					"    - `overwrite`: A flag indicating whether the existing data should be overwritten, assumed to be defined elsewhere.\n",
					"\n",
					"    Returns:\n",
					"    --------\n",
					"    str\n",
					"        The output log of the notebook run.\n",
					"\n",
					"    Notes:\n",
					"    ------\n",
					"    - The function assumes that `file_ingest_notebook_path`, `metadata_schema`, `table_category`, `pipeline_id`, and `overwrite` are predefined variables in the environment.\n",
					"    - The function has a maximum run time of 3600 seconds (1 hour) for the notebook execution.\n",
					"    \"\"\"\n",
					"    output_log=mssparkutils.notebook.run(file_ingest_notebook_path, 3600, {\"useRootDefaultLakehouse\": True,\n",
					"                                                            \"metadata_schema\": metadata_schema,\n",
					"                                                            \"target_table_category\": table_category,\n",
					"                                                            \"source_table_name\" : bronze_tables,\n",
					"                                                            \"pipeline_id\": pipeline_id,\n",
					"                                                            \"overwrite\": overwrite})\n",
					"\n",
					"    return output_log\n",
					"\n",
					"\n",
					""
				],
				"execution_count": 40
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define a helper function to handle exceptions\n",
					"def process_wrapper(table):\n",
					"    \"\"\"\n",
					"    Wrapper function for processing a bronze table with error handling.\n",
					"\n",
					"    This function attempts to process the specified bronze table by calling the `process_bronze_tables` function. \n",
					"    If an exception occurs during the processing, it catches the exception and returns an error message.\n",
					"\n",
					"    Parameters:\n",
					"    ----------\n",
					"    table : str\n",
					"        The name of the bronze table to be processed.\n",
					"\n",
					"    Returns:\n",
					"    --------\n",
					"    str\n",
					"        - The output from the `process_bronze_tables` function if successful.\n",
					"        - An error message indicating that the task resulted in an error if an exception is caught.\n",
					"\n",
					"    Exceptions:\n",
					"    -----------\n",
					"    Catches any exceptions that occur during the execution of `process_bronze_tables` and returns a formatted error message.\n",
					"\n",
					"    Notes:\n",
					"    ------\n",
					"    - The function assumes that `process_bronze_tables` is defined elsewhere and handles the actual processing of the table.\n",
					"    \"\"\"\n",
					"    try:\n",
					"        return process_bronze_tables(table)\n",
					"    except Exception as e:\n",
					"        return f\"Task resulted in an error: {str(e)}\""
				],
				"execution_count": 41
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def dim_process(dim_table):\n",
					"    \"\"\"\n",
					"    Processes a dimension table by executing either a Data Definition Language (DDL) or Data Manipulation Language (DML) \n",
					"    script based on the provided table metadata. The function performs various tasks, including fetching metadata, \n",
					"    running DDL or DML scripts, and handling concurrency conflicts.\n",
					"\n",
					"    Args:\n",
					"        dim_table (str): The fully qualified name of the dimension table in the format 'schema.table'.\n",
					"\n",
					"    Returns:\n",
					"        str: Success message if processing completes successfully. If an error occurs, returns the error message and step.\n",
					"\n",
					"    Raises:\n",
					"        Exception: If a concurrent update occurs or any other error is encountered, a specific error message is returned.\n",
					"\n",
					"    Steps:\n",
					"        1. Extract table metadata (e.g., business logic, DDL).\n",
					"        2. If a hydration script is specified, execute it.\n",
					"        3. If `overwrite` is enabled or the table doesn't exist, run the DDL script.\n",
					"        4. If business logic exists and overwrite is not set, execute the DML script.\n",
					"        5. Capture processing time and status for logging.\n",
					"\n",
					"    Notes:\n",
					"        This function expects external logging and notebook utilities to be available in the environment.\n",
					"    \"\"\"\n",
					"    try:\n",
					"\n",
					"        rows_processed = spark.sparkContext.accumulator(0)\n",
					"        process_start = datetime.datetime.now(eastern)\n",
					"\n",
					"        step = 'Get Dimension Table Metadata'\n",
					"\n",
					"        tableName = dim_table.split('.')[1].lower()\n",
					"        tableCategory = dim_table.split('.')[0].lower()\n",
					"        \n",
					"\n",
					"        tableInfo = table_def(tableCategory, tableName)\n",
					"        businessLogic = tableInfo['Business_Logic']\n",
					"        DDL = tableInfo['DDL']\n",
					"\n",
					"        if 'Hydration_Script' in DDL:\n",
					"            step = 'Runnning Hydration DDL'\n",
					"            mssparkutils.notebook.run(DDL, 3600, {\"table_name\": tableName })\n",
					"            status = 'SUCCESSFUL'\n",
					"            process_end = datetime.datetime.now(eastern)\n",
					"            #logTables.log_gold_process(pipeline_id,tableCategory,tableName,rows_processed,process_start,process_end,status)\n",
					"            return \n",
					"\n",
					"\n",
					"        \n",
					"        if overwrite=='yes' or businessLogic is None or spark.catalog.tableExists(dim_table)==False:\n",
					"            step = 'RUNNING DDL'\n",
					"            mssparkutils.notebook.run(DDL, 3600)\n",
					"        else:\n",
					"            step = 'RUNNING DML'\n",
					"            mssparkutils.notebook.run(businessLogic, 3600)\n",
					"\n",
					"        status = 'SUCCESSFUL'\n",
					"        process_end = datetime.datetime.now(eastern)\n",
					"        #logTables.log_gold_process(pipeline_id,tableCategory,tableName,rows_processed,process_start,process_end,status)\n",
					"\n",
					"        \n",
					"        return \"SUCCESS FOR \"+ dim_table\n",
					"\n",
					"    except Exception as e:\n",
					"\n",
					"        if 'The metadata of the Delta table has been changed by a concurrent update' in str(e):\n",
					"            print(f\"Table : {dim_table} Already Updated in Execution. No need to execute again\")\n",
					"            return\n",
					"        error_description = f\"Dimension loading process failed at: {step}\"\n",
					"        status = 'FAILED'\n",
					"        process_end = datetime.datetime.now(eastern)\n",
					"        #logTables.log_gold_process(pipeline_id,tableCategory,tableName,rows_processed,process_start,process_end,status)\n",
					"        #logTables.log_notebook_error(pipeline_id, 'gold', tableCategory, tableName, error_description, process_end)\n",
					"        print(f\"{dim_table}, {step}\")\n",
					"        return f\"{dim_table}, {step}\"\n",
					"\n",
					"#using this\n",
					"def process_table(table_name):\n",
					"    \"\"\"\n",
					"    Wrapper function for processing a dimension table.\n",
					"\n",
					"    Args:\n",
					"        table_name (str): Name of the dimension table.\n",
					"\n",
					"    Returns:\n",
					"        str: Result from `dim_process`, indicating either success or an error message.\n",
					"\n",
					"    Example:\n",
					"        process_table(\"schema.table_name\")\n",
					"    \"\"\"\n",
					"    return_value = dim_process(table_name.lower())\n",
					"    return return_value"
				],
				"execution_count": 42
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import re\n",
					"\n",
					"def remove_special_characters(message):\n",
					"    \"\"\"\n",
					"    Remove specific special characters from a given string.\n",
					"\n",
					"    This function removes backslashes, newline characters (`\\n`), and carriage return \n",
					"    characters (`\\r`) from the input string `message`.\n",
					"\n",
					"    Parameters:\n",
					"        message (str): The input string from which special characters should be removed.\n",
					"\n",
					"    Returns:\n",
					"        str: The modified string with specified special characters removed.\n",
					"    \n",
					"    Example:\n",
					"        >>> remove_special_characters(\"Hello\\\\ World\\nWelcome\\r!\")\n",
					"        'Hello WorldWelcome!'\n",
					"\n",
					"    Purpose: This function will remove the error when sending out EMAILS for large stack errors\n",
					"    \"\"\"\n",
					"    # Remove backslashes\n",
					"    message = re.sub(r'\\\\', '', message)\n",
					"    # Remove newline and carriage return characters\n",
					"    message = re.sub(r'[\\n\\r]', '', message)\n",
					"    return message\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def persist_cdf_logs(source_path, primary_key, bronze_name, cdf_path):\n",
					"    \"\"\"\n",
					"    Persist Change Data Feed (CDF) logs to a Delta table.\n",
					"\n",
					"    This function manages CDF logs by creating or updating a Delta table to store change data\n",
					"    from a specified source path. If the target Delta table does not exist, it creates one and saves\n",
					"    the data. If the table exists, it performs a merge operation to update it with new data.\n",
					"\n",
					"    Parameters:\n",
					"        source_path (str): The path to the source data directory, which should contain change data in Parquet format.\n",
					"        primary_key (str): A comma-separated string of primary key column names used to identify unique records.\n",
					"        bronze_name (str): The name of the bronze table; used to generate the target CDF table name.\n",
					"        cdf_path (str): The path where the Delta table for CDF logs will be stored.\n",
					"\n",
					"    Process:\n",
					"        - If the target Delta table in `cdf_logs` database doesn't exist, create it with the initial CDF data from `source_path`.\n",
					"        - If the Delta table already exists, perform a merge operation to update it with new change data.\n",
					"        The merge is based on matching `Id`, `commit_timestamp`, and `_change_type` columns.\n",
					"\n",
					"    \"\"\"\n",
					"\n",
					"    from pyspark.sql import functions as F\n",
					"    from delta.tables import DeltaTable\n",
					"\n",
					"    \n",
					"\n",
					"    primarykey = [x.strip() for x in primary_key.split(',')]\n",
					"    cdf_table_name = bronze_name.replace(\".\", \"_\") + \"_cdc\"\n",
					"    spark.sql(\"CREATE DATABASE IF NOT EXISTS cdf_logs\")\n",
					"    source_path_cdf = source_path + \"/_change_data\"\n",
					"\n",
					"    # Check if the target Delta table exists\n",
					"    if spark.catalog.tableExists(\"cdf_logs.\"+cdf_table_name)==False:\n",
					"        # Create the Delta table if it doesn't exist\n",
					"        try:\n",
					"            spark.read.parquet(source_path_cdf) \\\n",
					"                .withColumn(\"commit_timestamp\", (F.unix_timestamp(\"SinkCreatedOn\") * 1000).cast(\"bigint\")) \\\n",
					"                .withColumnRenamed(\"_change_type\",\"change_type\")\\\n",
					"                .write.format(\"delta\") \\\n",
					"                .mode(\"overwrite\") \\\n",
					"                .option(\"overwriteSchema\", True)\\\n",
					"                .save(cdf_path)\n",
					"            \n",
					"            spark.sql(f\"\"\"\n",
					"                CREATE TABLE IF NOT EXISTS cdf_logs.{cdf_table_name}\n",
					"                USING DELTA\n",
					"                LOCATION '{cdf_path}'\n",
					"            \"\"\")\n",
					"            spark.sql(f\"\"\"\n",
					"                ALTER TABLE delta.`{cdf_path}`\n",
					"                SET TBLPROPERTIES (\n",
					"                    'delta.logRetentionDuration' = 'interval 90 days',\n",
					"                    'delta.enableChangeDataFeed' = 'true'\n",
					"                )\n",
					"            \"\"\")\n",
					"\n",
					"\n",
					"\n",
					"\n",
					"        except Exception as e:\n",
					"            if \"PATH_NOT_FOUND\" in str(e):\n",
					"                print(\"Proceed, no CDF available yet for table :\"+ bronze_name)\n",
					"    else:\n",
					"\n",
					"        try:\n",
					"            source_df = spark.read.parquet(source_path_cdf) \\\n",
					"                .withColumn(\"commit_timestamp\", (F.unix_timestamp(\"SinkCreatedOn\") * 1000).cast(\"bigint\"))\\\n",
					"                .withColumnRenamed(\"_change_type\",\"change_type\")\n",
					"            \n",
					"            target_table = DeltaTable.forPath(spark, cdf_path)\n",
					"            \n",
					"            # Define the merge condition\n",
					"            merge_condition = \"target.Id = source.Id AND target.commit_timestamp = source.commit_timestamp and target.change_type = source.change_type\"\n",
					"            spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", True)\n",
					"            # Perform the merge with whenNotMatchedInsertAll\n",
					"            target_table.alias(\"target\").merge(\n",
					"                source_df.alias(\"source\"),\n",
					"                merge_condition\n",
					"            ).whenNotMatchedInsertAll().execute()\n",
					"        except Exception as e:\n",
					"            if \"PATH_NOT_FOUND\" or \"UNABLE_TO_INFER_SCHEMA\" in str(e):\n",
					"                print(\"Proceed, no more CDF available yet for table :\"+ bronze_name)\n",
					"\n",
					""
				],
				"execution_count": null
			}
		]
	}
}